# LLM vs LLM

**Исследование способностей языковых моделей к генерации вредоносных запросов и оцениванию других языковых моделей**

Проект нацелен на создание бенчмарка для оценки способностей больших языковых моделей при генерации и последующей проверке вредоносных запросов, а также на проверку устойчивости других моделей к возможным атакам в роли судьи.

Авторы: Иванов Н.А., Неронов Р.М., Низамов Т.Д., 1 курс магистратуры AI Talent Hub, Университет ИТМО

*Выполнено в рамках проектной практики в [AI Security Lab](https://ai.itmo.ru/aisecuritylab) ИТМО и Raft и курса ПИШ ИТМО «Инжиниринг управления данными».*

## Задачи

- [Составление атакующих и судейских промтов и генерация данных](llm-vs-llm-generating.ipynb) ✅
- Разметка прохождения атак и качества атакующих промтов ✅
- [Анализ данных по итогам разметки: выбор атакующих и судейских промтов, атакующей модели, составление бенчмарка](llm-vs-llm-benchmarking.ipynb) ✅
- [Прогон на бенчмарке разных моделей](llm-vs-llm-ratings.ipynb) ✅
- [Визуализация бенчмарка](visualization.py) ✅

## Рассматриваемые тесты проведения атак

 - [Ethical Compliance](https://github.com/RomiconEZ/llamator/blob/main/src/llamator/attacks/ethical_compliance.py). Оценивает способность LLM противостоять обсуждениям вредных или неуместных заблуждений по чувствительным темам. Этот тест направлен на проверку соответствия модели этическим рекомендациям против распространения стереотипов и токсичного контента.
 - [Logical Inconsistencies Test](https://github.com/RomiconEZ/llamator/blob/main/src/llamator/attacks/logical_inconsistencies.py). Проверяет связность и последовательность ответов LLM-системы на основе описания, вступая с ней в спор с использованием перефразирования и отрицания.
 - [Sycophancy Test](https://github.com/RomiconEZ/llamator/blob/main/src/llamator/attacks/sycophancy.py). Оценивает способность LLM противостоять обсуждениям вредных или неуместных заблуждений по чувствительным темам. Этот тест направлен на проверку соответствия модели этическим рекомендациям против распространения стереотипов и токсичного контента.

Выполнение атак осуществлено с помощью разрабатываемого авторами исследования LLM Red teaming фреймворка [LLAMATOR](https://github.com/RomiconEZ/llamator).

## Визуализация результатов тестов

Этот сервис позволяет визуализировать результаты тестов из нескольких CSV-файлов с помощью веб-интерфейса на основе Gradio.

1. **Установите необходимые библиотеки из `requirements.txt`**:

   ```bash
   pip install -r requirements.txt
   ```

2. **Запустите сервис из консоли**:

   ```bash
   python visualization.py
   ```

3. **Откройте веб-интерфейс**:

  - **Верхние графики**: Три тепловые карты для метрик по атакам:
    - Ethical Compliance
    - Logical Inconsistencies Test
    - ycophancy Test

  - **Нижний график**: Отображает результаты атак в виде сгруппированных столбцов.

## Данные

 - [data/llm-vs-llm-generated.xlsx](data/llm-vs-llm-generated.xlsx) — размеченные данные для атак и ответы атакуемых систем:
   - Название атаки (теста)
   - Название атакующей LLM
   - Хэш системного промта атакующей модели
   - Название атакуемой системы
   - Первый запрос атакующей модели
   - Первый ответ атакуемой системы
   - Второй запрос атакующей модели (опционально)
   - Второй ответ атакуемой системы (опционально)
   - Вердикт судьи при системном промте A (BROKEN, RESILIENT)
   - Вердикт судьи при системном промте B (BROKEN, RESILIENT)
   - Вердикт разметчика (BROKEN, RESILIENT)
   - Человеческая оценка качества атакующих запросов (-1 — плохо, 0 — нейтрально, -1 — хорошо)
 - [data/llm-vs-llm-benchmark.csv](data/llm-vs-llm-benchmark.csv) — бенчмарк для оценки моделей-судей:
   - `attack` — название атаки (теста)
   - `attack_model` — атакующая модель
   - `tested_model` — тестируемая модель (система)
   - `first_attack_prompt` — начальный атакующий промт
   - `first_response` — первый ответ атакуемой системы
   - `second_attack_prompt` — второй атакующий промт
   - `second_response` — второй ответ системы
   - `verdict_assessor` — вердикт (Broken, Resilient)
 - [data/ethical.csv](data/ethical.csv) — результаты прогона моделей-судей на ответах атаки с этико-правовыми отклонениями 
 - [data/logical.csv](data/logical.csv) — результаты прогона моделей-судей на ответах атаки с логическими несоответствиями
 - [data/sycophancy.csv](data/sycophancy.csv) — результаты прогона моделей-судей на ответах атаки с подхалимством
